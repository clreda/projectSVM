\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage[normalem]{ulem}
\usetheme{Antibes}

\title{Projet Optimisation\\Support Vector Machine}
\author{K. Kamtue \& Cl. Réda}
\institute{ENS Cachan}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\setlength{\parindent}{1cm}

\addtobeamertemplate{footline}{\insertframenumber / \inserttotalframenumber}

\section{Description du projet}

\begin{frame}
\frametitle{Implémentation d'un SVM}

\begin{center}
\textbf{Objectif :} Faire de l'apprentissage supervisé
\end{center}

\begin{itemize}
\item Appliqué à la classification binaire

         \begin{center}
         \incudegraphics[scale=0.3]{images/voronoi.png}
         \end{center}

\item Recherche d'une frontière linéaire $f$ vérifiant (condition 1) :

         \begin{center}
         $\forall i, y_i = -1 \Rightarrow f(x_i) \leq -1$\\
         $\forall i, y_i = 1 \Rightarrow f(x_i) \geq 1$\\
         $\Leftrightarrow \forall i, y_i \times f(x_i) \geq 1$
         \end{center}
\end{itemize}

\end{frame}

\section{Le problème d'optimisation}

\begin{frame}
\frametitle{Recherche du problème d'optimisation}

\begin{block}{Naïvement}

      \begin{center}
        $max_{w}$ $\gamma = \frac{2}{\|w\|}$ avec (1)\\
        $\Leftrightarrow min_{w}$ $\|w\|$ avec (1)\\
        $\Leftrightarrow min_{w}$ $\frac{1}{2} \times \|w\|^2$ avec (1)
      \end{center}

\end{block}

\end{frame}

\begin{frame}
\frametitle{Amélioration}

\begin{block}{En rendant le problème toujours faisable}

           \begin{centre}
           (P) $min_{w}$ $\frac{1}{2} \times \|w\| + C \times \sum_{i \leq m}z_i$\\
           avec $\forall i, z_i \geq 0$\\
           $\forall i, y_i \times (\omega^{T} x_i) \geq 1 - z_i$\\
           \end{centre}

\end{block}

\end{frame}

\section{Implémentation}

\begin{frame}
\frametitle{Résolution du problème d'optimisation}

\begin{itemize}
\item Utilisation de la méthode de Newton :

\begin{block}{Rappel}
          \begin{center}
          $\omega_{n+1} \leftarrow \omega_{n} + size \times \nabla^2 obj(\omega_n)^{-1}\nabla obj(\omega_n)$
          \end{center}

  (ici, en cherchant $size$ par \emph{backtracking line search})
\end{block}

\item Rendre le problème indépendant de la \emph{dimension}, ce qui permet d'appliquer cette méthode pour des problèmes à dimension de taille modérée (mais le problème sera dépendant du nombre d'échantillons !) en résolvant le \emph{problème dual}

\item Utiliser la méthode de la barrière logarithmique pour s'affranchir des contraintes d'inégalité

\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Le problème dual}

Après calcul du lagrangien et minimisation par rapport à $\omega$ et $z$ :

\begin{block}{Problème dual}
             \begin{center}
             $max_{\lambda \in \mathbb{R}^{+m}} -\frac{1}{2}\|\sum_i\lambda_iy_ix_i\|^2_2 + $\textbf{1}$^T\lambda$\\ 
             avec $\forall i, 0 \leq \lambda_i \leq C$ si $z_i > 0$\\
             \end{center}
\end{block}

\begin{block}{Obtenir la solution du primal à partir de celle du dual}
             \begin{center}
               $\omega^{*} = \sum_i \lambda^{*}_i y_i x_i$
             \end{center}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Rendre le problème indépendant de la dimension}

Utilisation de l'\emph{astuce du noyau} :

\begin{block}{Problème dual}
Soit $K = X^TX$. Alors :
                 \begin{center}
                 $max$ $-\frac{1}{2}\lambda^Tdiag(y)Kdiag(y)\lambda+$\textbf{1}$^T\lambda$\\
                 avec $\forall i, 0 \leq \lambda_i \leq C$ 
                 \end{center}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Supprimer les contraintes d'inégalité}

Utilisation de la \emph{méthode de la barrière logarithmique} :

\begin{block}{Fonction barrière}
          \begin{center}
          $\Phi(\lambda) = \sum_i (- log(C - \lambda_i) - log(\lambda_i)) = \sum_i log(\frac{1}{(C - \lambda_i)\lambda_i}) = - \sum_i log((C - \lambda_i)\lambda_i)$ 
          \end{center}
\end{block}

\begin{block}{Problème d'optimisation final}
          \begin{center}
          $max$ $-\frac{1}{2}\lambda^Tdiag(y)Kdiag(y)\lambda+$\textbf{1}$^T\lambda + \Phi(\lambda)$\\ 
          \end{center}
\end{block}

\end{frame}

\section{Résultats}

%TODO Mettre des figures

\section{Extensions}

\begin{frame}
\frametitle{Extensions}

\begin{itemize}
\item Validation croisée pour le choix de la meilleure valeur de C

\item %TODO
\end{itemize}

\end{frame}

%TODO Décrire les extensions

\end{document}
